  "day17": [
    {
      "id": 1,
      "topic": "Partial Deriv",
      "prompt": "Find $\\frac{\\partial f}{\\partial x}$ if $f(x, y) = x^2y + 5y^3$.",
      "answer": ["2xy"],
      "display": "$2xy$",
      "generate": function() {
        const a = GenUtils.randomInt(2, 6);
        const b = GenUtils.randomInt(2, 6);
        return {
          prompt: `Find $\\frac{\\partial f}{\\partial x}$ if $f(x, y) = x^{${a}}y + ${b}y^3$.`,
          answer: [`${a}x^{${a-1}}y`],
          display: `${a}x^{${a-1}}y`
        };
      }
    },
    {
      "id": 2,
      "topic": "Partial Deriv",
      "prompt": "Find $\\frac{\\partial f}{\\partial y}$ if $f(x, y) = x^2y + 5y^3$.",
      "answer": ["x^2 + 15y^2"],
      "display": "$x^2 + 15y^2$",
      "generate": function() {
        const a = GenUtils.randomInt(2, 6);
        const b = GenUtils.randomInt(2, 6);
        return {
          prompt: `Find $\\frac{\\partial f}{\\partial y}$ if $f(x, y) = x^{${a}}y + ${b}y^2$.`,
          answer: [`x^{${a}} + ${2*b}y`],
          display: `x^{${a}} + ${2*b}y`
        };
      }
    },
    {
      "id": 3,
      "topic": "Gradient",
      "prompt": "The gradient vector $\\nabla f$ points in the direction of steepest ____.",
      "answer": ["ascent", "increase", "uphill"],
      "display": "Ascent"
    },
    {
      "id": 4,
      "topic": "Gradient Descent",
      "prompt": "In ML, we update weights using $w = w - \\eta \\nabla L$. Why the minus sign?",
      "answer": ["to go downhill", "minimize loss", "descend"],
      "display": "To go downhill (minimize loss)"
    },
    {
      "id": 5,
      "topic": "Gradient",
      "prompt": "Find $\\nabla f$ at $(1, 1)$ for $f(x, y) = x^2 + y^2$.",
      "answer": ["(2, 2)", "[2, 2]"],
      "display": "$\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}$",
      "generate": function() {
        const x = GenUtils.randomInt(1, 5);
        const y = GenUtils.randomInt(1, 5);
        return {
          prompt: `Find $\\nabla f$ at (${x}, ${y}) for $f(x, y) = x^2 + y^2$.`,
          answer: [`(${2*x}, ${2*y})`],
          display: `\\begin{bmatrix} ${2*x} \\\\ ${2*y} \\end{bmatrix}`
        };
      }
    },
    {
      "id": 6,
      "topic": "Directional Deriv",
      "prompt": "The directional derivative $D_u f$ is the dot product of $\\nabla f$ and ___ vector $u$.",
      "answer": ["unit", "normalized"],
      "display": "Unit vector"
    },
    {
      "id": 7,
      "topic": "Jacobian",
      "prompt": "The Jacobian matrix is for functions where both input and output are ____.",
      "answer": ["vectors"],
      "display": "Vectors"
    },
    {
      "id": 8,
      "topic": "Hessian",
      "prompt": "The Hessian matrix contains the second-order ____ derivatives.",
      "answer": ["partial"],
      "display": "Partial"
    },
    {
      "id": 9,
      "topic": "Hessian",
      "prompt": "If the Hessian is positive-definite at a critical point, it is a local ____.",
      "answer": ["minimum", "min"],
      "display": "Minimum"
    },
    {
      "id": 10,
      "topic": "Jacobian",
      "prompt": "In Robotics, the Jacobian relates joint speeds to the velocity of the ___.",
      "answer": ["end-effector", "hand", "gripper"],
      "display": "End-effector (Hand)"
    },
    {
      "id": 11,
      "topic": "ML Connection",
      "prompt": "The 'Loss Landscape' of a neural net is a high-dimensional ____.",
      "answer": ["surface", "function", "manifold"],
      "display": "Surface (Manifold)"
    },
    {
      "id": 12,
      "topic": "Newton's Method",
      "prompt": "Newton's method for optimization uses the inverse of the ___ matrix.",
      "answer": ["hessian"],
      "display": "Hessian"
    },
    {
      "id": 13,
      "topic": "Partial Deriv",
      "prompt": "Is $\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}$ usually true? (yes/no)",
      "answer": ["yes", "y", "true"],
      "display": "Yes (Clairaut's Theorem)"
    },
    {
      "id": 14,
      "topic": "Gradient",
      "prompt": "The magnitude of the gradient $|\\nabla f|$ represents the ____ of the slope.",
      "answer": ["steepness", "slope", "rate"],
      "display": "Steepness"
    },
    {
      "id": 15,
      "topic": "Concept",
      "prompt": "Level sets of $f(x, y) = c$ are always _____ to the gradient vector.",
      "answer": ["orthogonal", "perpendicular"],
      "display": "Orthogonal (Perpendicular)"
    },
    {
      "id": 16,
      "topic": "Chain Rule",
      "prompt": "Multivariable chain rule for $f(x(t), y(t))$ is $df/dt = (\\partial f/\\partial x)(dx/dt) + (___)(___)$.",
      "answer": ["(partial f/partial y)(dy/dt)", "df/dy*dy/dt"],
      "display": "$\\frac{\\partial f}{\\partial y}\\frac{dy}{dt}$"
    },
    {
      "id": 17,
      "topic": "Optimization",
      "prompt": "Lagrange Multipliers are used to find extrema under ____.",
      "answer": ["constraints", "constraint"],
      "display": "Constraints"
    },
    {
      "id": 18,
      "topic": "Jacobian",
      "prompt": "The determinant of the Jacobian represents the local ____ factor of a transformation.",
      "answer": ["scaling", "area", "volume"],
      "display": "Scaling (Area/Volume)"
    },
    {
      "id": 19,
      "topic": "Hessian",
      "prompt": "A saddle point occurs when the Hessian has both positive and negative ____.",
      "answer": ["eigenvalues"],
      "display": "Eigenvalues"
    },
    {
      "id": 20,
      "topic": "PyTorch",
      "prompt": "In PyTorch, calling `.backward()` on a scalar loss populates the `.____` attribute of the weights.",
      "answer": ["grad"],
      "display": "grad"
    },
    {
      "id": 21,
      "topic": "Latent Space",
      "prompt": "Moving through 'Latent Space' in an LLM is like calculating gradients in a high-dimensional ____.",
      "answer": ["vector space", "manifold"],
      "display": "Vector Space (Manifold)"
    },
    {
      "id": 22,
      "topic": "Partial Deriv",
      "prompt": "If $f = g(x) + h(y)$, then $\\frac{\\partial^2 f}{\\partial x \\partial y} = ___$.",
      "answer": ["0"],
      "display": "0"
    },
    {
      "id": 23,
      "topic": "Gradient",
      "prompt": "If $\\nabla f = 0$ at a point, it is a _____ point.",
      "answer": ["critical", "stationary"],
      "display": "Critical (Stationary)"
    },
    {
      "id": 24,
      "topic": "Directional Deriv",
      "prompt": "The directional derivative is maximized when the direction $u$ is parallel to the ___.",
      "answer": ["gradient", "nabla f"],
      "display": "Gradient"
    },
    {
      "id": 25,
      "topic": "Hessian",
      "prompt": "Second derivative test in 2D uses the determinant of the ____.",
      "answer": ["hessian", "hessian matrix"],
      "display": "Hessian Matrix"
    },
    {
      "id": 26,
      "topic": "ML",
      "prompt": "Stochastic Gradient Descent (SGD) uses a _____ of data to estimate the gradient.",
      "answer": ["subset", "minibatch", "batch"],
      "display": "Subset (Minibatch)"
    },
    {
      "id": 27,
      "topic": "Concept",
      "prompt": "A scalar field $f(x, y, z)$ maps vectors to ____.",
      "answer": ["scalars", "numbers"],
      "display": "Scalars"
    },
    {
      "id": 28,
      "topic": "Concept",
      "prompt": "A vector field $F(x, y, z)$ maps vectors to ____.",
      "answer": ["vectors"],
      "display": "Vectors"
    },
    {
      "id": 29,
      "topic": "Notation",
      "prompt": "$\\nabla^2 f$ (Laplacian) is the ____ of the gradient.",
      "answer": ["divergence"],
      "display": "Divergence"
    },
    {
      "id": 30,
      "topic": "Jacobian",
      "prompt": "The Jacobian matrix is usually denoted by capitalized letter ___.",
      "answer": ["j"],
      "display": "J"
    },
    {
      "id": 31,
      "topic": "Partial Deriv",
      "prompt": "$f(x,y)=e^{xy}$. Find $f_x$.",
      "answer": ["ye^{xy}", "ye^xy"],
      "display": "$ye^{xy}$"
    },
    {
      "id": 32,
      "topic": "Partial Deriv",
      "prompt": "$f(x,y)=\\sin(x+y)$. Find $f_x$.",
      "answer": ["cos(x+y)"],
      "display": "$\\cos(x+y)$"
    },
    {
      "id": 33,
      "topic": "Gradient",
      "prompt": "Magnitude of $\\nabla f$ at origin for $f=x^2+y^2$ is ___.",
      "answer": ["0"],
      "display": "0"
    },
    {
      "id": 34,
      "topic": "Gradient",
      "prompt": "Magnitude of $\\nabla f$ at $(1,0)$ for $f=x^2+y^2$ is ___.",
      "answer": ["2"],
      "display": "2"
    },
    {
      "id": 35,
      "topic": "Jacobian",
      "prompt": "For $f(x,y) = (x+y, xy)$, the Jacobian at $(1,2)$ is [[1,1], [?,?]]. What are the bottom row values?",
      "answer": ["2,1", "2 1"],
      "display": "2, 1"
    },
    {
      "id": 36,
      "topic": "Hessian",
      "prompt": "Hessian of $x^2+y^2$ is the constant matrix [[?,?],[?,?]]. Values are?",
      "answer": ["2,0,0,2"],
      "display": "[[2,0],[0,2]]"
    },
    {
      "id": 37,
      "topic": "Stability",
      "prompt": "In high dimensions, most critical points are ____ points, not minima.",
      "answer": ["saddle"],
      "display": "Saddle"
    },
    {
      "id": 38,
      "topic": "Directional Deriv",
      "prompt": "If $u$ is perpendicular to $\\nabla f$, the directional derivative is ___.",
      "answer": ["0"],
      "display": "0"
    },
    {
      "id": 39,
      "topic": "Technique",
      "prompt": "The 'Automatic Differentiation' in TensorFlow uses the ___ rule of Jacobians.",
      "answer": ["chain"],
      "display": "Chain"
    },
    {
      "id": 40,
      "topic": "Conclusion",
      "prompt": "Multivariable calculus is the bridge between 1D math and real-world ___.",
      "answer": ["ai", "robotics", "data science"],
      "display": "AI/Data Science"
    }
  ],
