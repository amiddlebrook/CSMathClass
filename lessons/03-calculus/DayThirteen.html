<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Day 13: The Chain Rule</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500;600;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <link rel="stylesheet" href="../../lessons/shared-styles.css">
    <script src="../../lessons/questions-data.js"></script>
    <script src="../../lessons/shared-scripts.js"></script>
</head>

<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../../index.html" class="nav-back">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M10 12L6 8L10 4" />
                </svg>
                Back to Curriculum
            </a>
            <div class="nav-progress">
                <span id="progressText">0 / 20 complete</span>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
            </div>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" />
                </svg>
            </button>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-label">
            <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                <path d="M8 0L10 6H16L11 9.5L13 16L8 12L3 16L5 9.5L0 6H6L8 0Z" />
            </svg>
            Day 13 ¬∑ The Engine of AI
        </div>
        <h1>The Chain Rule</h1>
        <p class="hero-desc">
            The Chain Rule is likely the most profitable equation in history.
            It is the mechanism behind Backpropagation, which trains every modern Neural Network like ChatGPT.
        </p>
        <div class="hero-meta">
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <circle cx="12" cy="12" r="10" />
                    <path d="M12 6v6l4 2" />
                </svg>
                ~120 min read
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path
                        d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2" />
                </svg>
                45 practice problems
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <polygon points="5 3 19 12 5 21 5 3" />
                </svg>
                12 video lessons
            </div>
        </div>
    </header>

    <div class="main-layout">
        <aside class="sidebar">
            <nav class="toc">
                <div class="toc-title">On this page</div>
                <ul class="toc-list">
                    <li><a href="#why" class="toc-link">Why This Matters</a></li>
                    <li><a href="#definition" class="toc-link">1. The Rule</a></li>
                    <li><a href="#intuition" class="toc-link">2. Intuition (Gears)</a></li>
                    <li><a href="#examples" class="toc-link">3. Worked Examples</a></li>
                    <li><a href="#backprop" class="toc-link">4. CS: Backpropagation</a></li>
                    <li><a href="#graph" class="toc-link">5. CS: Comp Graphs</a></li>
                    <li><a href="#autodiff" class="toc-link">6. Automatic Differentiation</a></li>
                    <li><a href="#python" class="toc-link">7. Python Code</a></li>
                    <li><a href="#summary" class="toc-link">8. Summary</a></li>
                    <li><a href="#videos" class="toc-link">9. Video Lessons</a></li>
                    <li><a href="#practice" class="toc-link">10. Practice Problems</a></li>
                </ul>
            </nav>
            <div class="stats-card">
                <div class="toc-title">Your Progress</div>
                <div class="stats-row"><span class="stats-label">Attempted</span><span class="stats-value"
                        id="statAttempted">0</span></div>
                <div class="stats-row"><span class="stats-label">Correct</span><span class="stats-value"
                        id="statCorrect">0</span></div>
                <div class="stats-row"><span class="stats-label">Accuracy</span><span class="stats-value"
                        id="statAccuracy">‚Äî</span></div>
            </div>
        </aside>

        <main class="content">
            <!-- Why This Matters -->
            <section class="section" id="why">
                <div class="section-header">
                    <div class="section-number">Section 0</div>
                    <h2 class="section-title">Why the Chain Rule Matters</h2>
                </div>
                <div class="section-body">
                    <p>
                        Most real-world functions are compositions.
                        "The price of oil affects gas, which affects shipping, which affects the price of your Amazon
                        package."
                    </p>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>üß† Deep Learning</h3>
                            <p>Neural Networks are composed functions: layers upon layers.</p>
                            <p>Training uses <strong>Backpropagation</strong>‚Äîthe Chain Rule applied recursively.</p>
                            <p>Every AI model (GPT, DALL-E, AlphaFold) depends on this.</p>
                        </div>
                        <div class="card">
                            <h3>üí∞ Finance</h3>
                            <p>Option prices depend on stock prices depend on market factors.</p>
                            <p>The "Greeks" (Delta, Gamma) are chain rule applications.</p>
                            <p>Trillions of dollars in risk managed via derivatives of derivatives.</p>
                        </div>
                    </div>

                    <div class="info">
                        <span class="info-icon">üîó</span>
                        <strong>The Core Insight:</strong>
                        <br>A Deep Neural Network is just a massive composition of functions:
                        <br>$y = f(g(h(i(...(x)))))$
                        <br>To know how "x" affects "y", you need the Chain Rule applied at every link in the chain.
                    </div>
                </div>
            </section>

            <!-- Definition -->
            <section class="section" id="definition">
                <div class="section-header">
                    <div class="section-number">Section 1</div>
                    <h2 class="section-title">The Rule</h2>
                </div>
                <div class="section-body">
                    <div class="math-block">
                        $$ \frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x) $$
                        Or in Leibniz notation (cleaner):
                        $$ \frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} $$
                    </div>
                    <div class="rule">
                        <strong>The Mantra:</strong>
                        Derivative of the OUTSIDE ... times ... Derivative of the INSIDE.
                    </div>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>Why Leibniz Notation is Nice</h3>
                            <p>The $du$ terms "cancel" like fractions:</p>
                            <p>$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$</p>
                            <p>This isn't rigorously correct, but it's a useful mnemonic.</p>
                        </div>
                        <div class="card">
                            <h3>Multi-Variable Extension</h3>
                            <p>For $y = f(g(x), h(x))$:</p>
                            <p>$\frac{dy}{dx} = \frac{\partial f}{\partial g}\frac{dg}{dx} + \frac{\partial f}{\partial
                                h}\frac{dh}{dx}$</p>
                            <p>This is the "fan-out" pattern in neural nets.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Intuition -->
            <section class="section" id="intuition">
                <div class="section-header">
                    <div class="section-number">Section 2</div>
                    <h2 class="section-title">Intuition: Gears & Pipelines</h2>
                </div>
                <div class="section-body">
                    <div class="grid-2col">
                        <div class="info">
                            <span class="info-icon">‚öôÔ∏è</span>
                            <strong>Analogy: The Gearbox</strong>
                            <br>Imagine 3 gears connected: A ‚Üí B ‚Üí C
                            <br>‚Ä¢ Gear B spins 2x faster than A: $\frac{dB}{dA} = 2$
                            <br>‚Ä¢ Gear C spins 3x faster than B: $\frac{dC}{dB} = 3$
                            <br>How much faster does C spin vs A?
                            <br>$\frac{dC}{dA} = \frac{dC}{dB} \times \frac{dB}{dA} = 3 \times 2 = 6$
                            <br><br>
                            <strong>In CS terms:</strong> If you double the CPU clock speed (A), and your code processes
                            3 pixels per clock (B), your total pixels per second (C) sextuples.
                        </div>
                        <div class="info">
                            <span class="info-icon">üö≤</span>
                            <strong>Analogy: The Bicycle</strong>
                            <br>When you pedal, your legs move the crank (A), which moves the chain (B), which turns the
                            wheel (C).
                            <br>‚Ä¢ $\frac{d(\text{Chain})}{d(\text{Crank})}$ is the front gear ratio.
                            <br>‚Ä¢ $\frac{d(\text{Wheel})}{d(\text{Chain})}$ is the rear gear ratio.
                            <br>Success in a race depends on the product of these ratios. To know how a 1cm leg movement
                            affects the bike's speed, you multiply the sensitivities of every link.
                        </div>
                    </div>

                    <div class="grid-2col">
                        <div class="info">
                            <span class="info-icon">üè≠</span>
                            <strong>Analogy: The Factory Pipeline</strong>
                            <br>Raw Materials ‚Üí Station A ‚Üí Station B ‚Üí Product
                            <br>If Materials ‚Üë10%: Station A output ‚Üë5%
                            <br>If Station A ‚Üë5%: Station B output ‚Üë20%
                            <br>Total effect on Product?
                            <br>0.05 √ó 0.20 = 0.01 = 1% per 10% input change
                        </div>
                        <div class="info">
                            <span class="info-icon">üé®</span>
                            <strong>Analogy: Digital Filters</strong>
                            <br>Imagine an image passing through: [Blur] ‚Üí [Brighten] ‚Üí [Contrast].
                            <br>‚Ä¢ If you slightly change the Blur radius, it affects the Brightness's input, which
                            affects the Contrast's input.
                            <br>‚Ä¢ To find how much a pixel final color changes relative to the blur radius, you multiply
                            the derivatives of each filter.
                        </div>
                    </div>

                    <div class="info">
                        <span class="info-icon">üßä</span>
                        <strong>Analogy: The Ice Cube</strong>
                        <br>An ice cube's volume depends on its side length: $V = s^3$.
                        <br>The side length depends on temperature: $s = f(T)$.
                        <br>How does volume change with temperature?
                        <br>$\frac{dV}{dT} = \frac{dV}{ds} \cdot \frac{ds}{dT} = 3s^2 \cdot f'(T)$
                    </div>
                </div>
            </section>

            <!-- Worked Examples -->
            <section class="section" id="examples">
                <div class="section-header">
                    <div class="section-number">Section 3</div>
                    <h2 class="section-title">Worked Examples</h2>
                </div>
                <div class="section-body">
                    <div class="subsection">
                        <h3 class="subsection-title">Example 1: $(x^2 + 1)^5$</h3>
                        <p><strong>Step 1:</strong> Identify outer and inner functions.</p>
                        <p>Outer: $f(u) = u^5$ ‚Üí $f'(u) = 5u^4$</p>
                        <p>Inner: $g(x) = x^2 + 1$ ‚Üí $g'(x) = 2x$</p>
                        <p><strong>Step 2:</strong> Apply Chain Rule.</p>
                        <div class="math-block">
                            $$ \frac{d}{dx}(x^2 + 1)^5 = 5(x^2+1)^4 \cdot 2x = 10x(x^2+1)^4 $$
                        </div>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Example 2: $\sin(x^3)$</h3>
                        <p>Outer: $\sin(u)$ ‚Üí $\cos(u)$</p>
                        <p>Inner: $u = x^3$ ‚Üí $3x^2$</p>
                        <div class="math-block">
                            $$ \frac{d}{dx}\sin(x^3) = \cos(x^3) \cdot 3x^2 = 3x^2\cos(x^3) $$
                        </div>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Example 3: $e^{x^2}$ (Double Chain!)</h3>
                        <p>Outer: $e^u$ ‚Üí $e^u$</p>
                        <p>Inner: $u = x^2$ ‚Üí $2x$</p>
                        <div class="math-block">
                            $$ \frac{d}{dx}e^{x^2} = e^{x^2} \cdot 2x = 2xe^{x^2} $$
                        </div>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Example 4: Triple Composition $\sin(e^{x^2})$</h3>
                        <p>Let $u = x^2$, $v = e^u$, $y = \sin(v)$</p>
                        <div class="math-block">
                            $$ \frac{dy}{dx} = \frac{dy}{dv} \cdot \frac{dv}{du} \cdot \frac{du}{dx} = \cos(v) \cdot e^u
                            \cdot 2x = 2x \cdot e^{x^2} \cdot \cos(e^{x^2}) $$
                        </div>
                    </div>

                    <div class="warning">
                        <strong>Common Mistake:</strong>
                        <br>Forgetting the "inside derivative"! When you see a composed function, always ask: what's
                        inside?
                        <br>Wrong: $\frac{d}{dx}\sin(x^2) = \cos(x^2)$ ‚ùå
                        <br>Right: $\frac{d}{dx}\sin(x^2) = \cos(x^2) \cdot 2x$ ‚úì
                    </div>
                </div>
            </section>

            <!-- Backprop -->
            <section class="section" id="backprop">
                <div class="section-header">
                    <div class="section-number">Section 4</div>
                    <h2 class="section-title">CS Application: Backpropagation Walkthrough</h2>
                </div>
                <div class="section-body">
                    <p>
                        This is the algorithm that "trains" AI.
                        We have a huge error at the end (Loss). We want to blame the weights at the beginning.
                    </p>

                    <div class="subsection">
                        <h3 class="subsection-title">Step-by-Step Backprop</h3>
                        <p>Consider a toy neuron: $y = \text{ReLU}(w \cdot x + b)$. We want to find
                            $\frac{d\text{Loss}}{dw}$.</p>

                        <div class="grid-2col">
                            <div class="card">
                                <h4>1. Forward Pass</h4>
                                <p>‚Ä¢ Compute $z = w \cdot x + b$</p>
                                <p>‚Ä¢ Compute output $y = \text{ReLU}(z)$</p>
                                <p>‚Ä¢ Compute Loss $L = (y - \text{target})^2$</p>
                            </div>
                            <div class="card">
                                <h4>2. Backward Pass (The Chain)</h4>
                                <p>‚Ä¢ $\frac{dL}{dy} = 2(y - \text{target})$ (Upstream)</p>
                                <p>‚Ä¢ $\frac{dy}{dz} = 1$ if $z > 0$, else $0$ (Local)</p>
                                <p>‚Ä¢ $\frac{dz}{dw} = x$ (Local)</p>
                            </div>
                        </div>

                        <div class="info">
                            <span class="info-icon">üßÆ</span>
                            <strong>The Multiplication:</strong>
                            <br>$\frac{dL}{dw} = \underbrace{\frac{dL}{dy}}_{\text{Loss gradient}} \times
                            \underbrace{\frac{dy}{dz}}_{\text{Activation gradient}} \times
                            \underbrace{\frac{dz}{dw}}_{\text{Weight gradient}}$
                            <br>If result is positive, increasing $w$ increases error. So we decrease $w$.
                        </div>
                    </div>

                    <div class="card">
                        <h3>Why "Backwards"?</h3>
                        <p>In a giant network, many weights share the same final layers. By starting at the end and
                            moving back, we calculate the "Downstream" gradients Once and reuse them for every weight
                            earlier in the chain. This turns an $O(N^2)$ problem into $O(N)$.</p>
                    </div>
                </div>
            </section>

            <!-- Comp Graph -->
            <section class="section" id="graph">
                <div class="section-header">
                    <div class="section-number">Section 5</div>
                    <h2 class="section-title">Computational Graphs</h2>
                </div>
                <div class="section-body">
                    <p>In TensorFlow or PyTorch, you build a "Graph" of operations.</p>

                    <div class="info">
                        <span class="info-icon">üìä</span>
                        <strong>Analogy: The Data Flow Diagram</strong>
                        <br>x ‚Üí [Multiply by 2] ‚Üí u ‚Üí [Square] ‚Üí y
                        <br>Each arrow is a "local derivative" we can compute.
                        <br>To find dy/dx, we traverse this graph backwards, multiplying local gradients.
                    </div>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>Forward Mode</h3>
                            <p>Compute derivatives left-to-right.</p>
                            <p>Good for: few inputs, many outputs.</p>
                            <p>Example: ‚àÇy/‚àÇx‚ÇÅ, ‚àÇy/‚àÇx‚ÇÇ, ... simultaneously.</p>
                        </div>
                        <div class="card">
                            <h3>Reverse Mode (Backprop)</h3>
                            <p>Compute derivatives right-to-left.</p>
                            <p>Good for: many inputs, few outputs.</p>
                            <p>Neural Nets: millions of weights, one loss ‚Üí Reverse Mode wins!</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Autodiff -->
            <section class="section" id="autodiff">
                <div class="section-header">
                    <div class="section-number">Section 6</div>
                    <h2 class="section-title">Automatic Differentiation & Dual Numbers</h2>
                </div>
                <div class="section-body">
                    <p>Modern ML frameworks (PyTorch, TensorFlow, JAX) implement autodiff automatically. But how does it
                        actually work?</p>

                    <div class="subsection">
                        <h3 class="subsection-title">Mechanism 1: Dual Numbers</h3>
                        <p>Think of $(a + b\epsilon)$ where $\epsilon^2 = 0$. This is like a complex number but for
                            derivatives.</p>
                        <div class="info">
                            If you pass $(x + \epsilon)$ into a function $f$, the result is $f(x) + f'(x)\epsilon$.
                            <br>The computer calculates the value and the derivative simultaneously in a single pass.
                        </div>
                    </div>

                    <div class="grid-2col">
                        <div class="info">
                            <span class="info-icon">üéØ</span>
                            <strong>Key Insight:</strong>
                            <br>You never write derivatives by hand.
                            <br>The framework builds the computation graph during forward pass,
                            <br>then automatically computes gradients via backprop.
                        </div>
                        <div class="info">
                            <span class="info-icon">‚ö°</span>
                            <strong>Why It's Efficient:</strong>
                            <br>Each local gradient is computed exactly once.
                            <br>Results are cached and reused.
                            <br>Complexity: O(cost of forward pass).
                        </div>
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">PyTorch Autograd Example</span>
                        </div>
                        <div class="code-content">
                            <pre><code class="language-python">import torch

# Create tensors with gradient tracking
x = torch.tensor(3.0, requires_grad=True)

# Build computation graph (forward pass)
u = 2 * x + 1      # u = 7
y = u ** 2         # y = 49

# Compute gradients (backward pass)
y.backward()

# The gradient is automatically computed!
print(f"dy/dx = {x.grad}")  # 28.0

# Compare to our manual calculation:
# y = (2x+1)¬≤, dy/dx = 2(2x+1)(2) = 4(2x+1) = 4(7) = 28 ‚úì</code></pre>
                        </div>
                    </div>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">JAX Functional Autodiff</span>
                        </div>
                        <div class="code-content">
                            <pre><code class="language-python">import jax
import jax.numpy as jnp

def f(x):
    return (2*x + 1) ** 2

# jax.grad returns a function that computes the gradient
df = jax.grad(f)

print(f"f(3) = {f(3.0)}")      # 49.0
print(f"f'(3) = {df(3.0)}")    # 28.0

# You can even get higher derivatives!
d2f = jax.grad(jax.grad(f))
print(f"f''(3) = {d2f(3.0)}")  # 8.0 (constant!)</code></pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Python -->
            <section class="section" id="python">
                <div class="section-header">
                    <div class="section-number">Section 7</div>
                    <h2 class="section-title">Python: Building a Scalar Autodiff Engine</h2>
                </div>
                <div class="section-body">
                    <p>To truly understand the Chain Rule, let's build the world's smallest Autodiff engine (inspired by
                        Karpathy's <code>micrograd</code>).</p>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Mini-Value Class</span>
                        </div>
                        <div class="code-content">
                            <pre><code class="language-python">class Value:
    def __init__(self, data, _children=()):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)

    def __add__(self, other):
        out = Value(self.data + other.data, (self, other))
        def _backward():
            self.grad += 1.0 * out.grad
            other.grad += 1.0 * out.grad
        out._backward = _backward
        return out

    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other))
        def _backward():
            self.grad += other.data * out.grad # dy/da = b
            other.grad += self.data * out.grad # dy/db = a
        out._backward = _backward
        return out

    def backward(self):
        self.grad = 1.0
        # Simple topological sort (reversed) would go here
        # For this example, we'll just call them manually or assume a chain
        self._backward()
        for child in self._prev:
            child.backward()

# Example: y = a * b + c
a, b, c = Value(2.0), Value(3.0), Value(10.0)
y = a * b + c 
y.backward()

print(f"dy/da = {a.grad}") # b = 3.0
print(f"dy/dc = {c.grad}") # 1.0</code></pre>
                        </div>
                    </div>

                    <div class="grid-2col">
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-lang">Python</span>
                                <span class="code-label">PyTorch Verification</span>
                            </div>
                            <div class="code-content">
                                <pre><code class="language-python">import torch

x = torch.tensor(3.0, requires_grad=True)
u = 2 * x + 1
y = u ** 2

y.backward()
print(f"dy/dx = {x.grad}") # 28.0</code></pre>
                            </div>
                        </div>
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-lang">Python</span>
                                <span class="code-label">SymPy (Symbolic)</span>
                            </div>
                            <div class="code-content">
                                <pre><code class="language-python">import sympy as sp

x = sp.Symbol('x')
f = (2*x + 1)**2
df = sp.diff(f, x)

print(f"Derivative: {df}") 
# 4*(2*x + 1)</code></pre>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Summary -->
            <section class="section" id="summary">
                <div class="section-header">
                    <div class="section-number">Section 8</div>
                    <h2 class="section-title">Summary: The Engine of Learning</h2>
                </div>
                <div class="section-body">
                    <p>The Chain Rule is the mathematical foundation that makes modern AI possible.</p>

                    <div class="subsection">
                        <h3 class="subsection-title">Glossary</h3>
                        <div class="grid-2col">
                            <div class="rule">
                                <strong>Core Concepts:</strong>
                                <br>‚Ä¢ <strong>Chain Rule:</strong> $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$
                                <br>‚Ä¢ <strong>Composition:</strong> $f(g(x))$ - functions inside functions
                                <br>‚Ä¢ <strong>Local Gradient:</strong> derivative at one node
                                <br>‚Ä¢ <strong>Upstream Gradient:</strong> gradient flowing back from output
                            </div>
                            <div class="rule">
                                <strong>Applications:</strong>
                                <br>‚Ä¢ <strong>Backpropagation:</strong> training neural networks
                                <br>‚Ä¢ <strong>Autodiff:</strong> automatic gradient computation
                                <br>‚Ä¢ <strong>Computational Graphs:</strong> dataflow diagrams
                                <br>‚Ä¢ <strong>Sensitivity Analysis:</strong> how inputs affect outputs
                            </div>
                        </div>
                    </div>

                    <div class="warning">
                        <strong>Common Gotchas:</strong>
                        <br>‚Ä¢ <strong>Vanishing Gradients:</strong> deep networks multiply many small numbers ‚Üí
                        gradients ‚Üí 0.
                        <br>‚Ä¢ <strong>Exploding Gradients:</strong> deep networks multiply many large numbers ‚Üí
                        gradients ‚Üí ‚àû.
                        <br>‚Ä¢ <strong>Non-differentiable Operations:</strong> Can't backprop through argmax, round,
                        if-statements.
                    </div>

                    <div class="info">
                        <span class="info-icon">üéì</span>
                        <strong>Mastery Quote:</strong>
                        <br>"The Chain Rule is the algorithm that powers trillion-dollar companies.
                        Every time you use ChatGPT, billions of chain rule applications happen behind the scenes."
                    </div>
                </div>
            </section>


            <!-- Videos -->
            <section class="section" id="videos">
                <div class="section-header">
                    <div class="section-number">Section 9</div>
                    <h2 class="section-title">Video Lessons</h2>
                </div>
                <div class="section-body">
                    <div class="video-container">
                        <div class="video-player">
                            <iframe id="ytPlayer" src="" allowfullscreen></iframe>
                            <div class="video-info">
                                <div class="video-title" id="videoTitle">Select a video</div>
                                <div class="video-meta" id="videoMeta"></div>
                            </div>
                        </div>
                        <div class="video-list" id="videoList"></div>
                    </div>
                </div>
            </section>

            <!-- Practice -->
            <section class="section" id="practice">
                <div class="section-header">
                    <div class="section-number">Section 10</div>
                    <h2 class="section-title">Practice Problems</h2>
                </div>
                <div class="section-body">
                    <div class="quiz-controls">
                        <button class="btn btn-primary" id="btnCheckAll">Check All</button>
                        <button class="btn" id="btnRevealAll">Reveal All</button>
                        <button class="btn" id="btnClear">Clear Inputs</button>
                        <button class="btn btn-danger" id="btnReset">Reset Stats</button>
                    </div>
                    <div id="quizContainer"></div>
                </div>
            </section>
        </main>
    </div>

    <script>
        const VIDEO_GROUPS = [
            {
                title: "Chain Rule Basics",
                items: [
                    { title: "Chain Rule Visualized", channel: "3Blue1Brown", vid: "YG15m2VwSjA" },
                    { title: "Chain Rule Examples", channel: "The Organic Chemistry Tutor", vid: "HaHsqDjWMLU" },
                    { title: "Nested Functions", channel: "Professor Leonard", vid: "HxVn6kRD5NM" },
                    { title: "Derivative & Notation", channel: "Khan Academy", vid: "EKvHQc3QEow" }
                ]
            },
            {
                title: "Backpropagation & AI",
                items: [
                    { title: "What is Backprop", channel: "3Blue1Brown", vid: "Ilg3gGewQ5U" },
                    { title: "Backprop Calculus", channel: "3Blue1Brown", vid: "tIeHLnjs5U8" },
                    { title: "Gradient Descent Intuition", channel: "StatQuest", vid: "sDv4f4s2SB8" },
                    { title: "Neural Network Math", channel: "StatQuest", vid: "w8yWXqWQYmU" }
                ]
            },
            {
                title: "Autodiff & Implementation",
                items: [
                    { title: "Automatic Differentiation", channel: "Computerphile", vid: "wG_nF1awSSY" },
                    { title: "PyTorch Autograd", channel: "Aladdin Persson", vid: "x9JiIFvlUwk" },
                    { title: "Building Micrograd", channel: "Andrej Karpathy", vid: "VMj-3S1tku0" },
                    { title: "JAX Ecosystem", channel: "Google Cloud Tech", vid: "r3C_iA1RzQc" }
                ]
            }
        ];


        initLesson({
            videos: VIDEO_GROUPS,
            questions: window.QUESTIONS_DATA['day13'] || [],
            storageKey: 'day13_v2'
        });
    </script>
    <script>hljs.highlightAll();</script>
</body>

</html>