<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Day 17: Multivariable Calculus</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500;600;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <link rel="stylesheet" href="../../lessons/shared-styles.css">
    <script src="../../lessons/questions-data.js"></script>
    <script src="../../lessons/shared-scripts.js"></script>
</head>

<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../../index.html" class="nav-back">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M10 12L6 8L10 4" />
                </svg>
                Back to Curriculum
            </a>
            <div class="nav-progress">
                <span id="progressText">0 / 20 complete</span>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
            </div>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" />
                </svg>
            </button>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-label">
            <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                <path d="M8 0L10 6H16L11 9.5L13 16L8 12L3 16L5 9.5L0 6H6L8 0Z" />
            </svg>
            Day 17 ¬∑ The High Dimensional World
        </div>
        <h1>Multivariable Calculus</h1>
        <p class="hero-desc">
            The real world (and AI) is not $f(x)$. It is $f(x, y, z, ...)$.
            When you have a million inputs, the Derivative becomes the Gradient.
        </p>
        <div class="hero-meta">
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <circle cx="12" cy="12" r="10" />
                    <path d="M12 6v6l4 2" />
                </svg>
                ~100 min read
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path
                        d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2" />
                </svg>
                40 practice problems
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <polygon points="5 3 19 12 5 21 5 3" />
                </svg>
                12 video lessons
            </div>
        </div>
    </header>

    <div class="main-layout">
        <aside class="sidebar">
            <nav class="toc">
                <div class="toc-title">On this page</div>
                <ul class="toc-list">
                    <li><a href="#why" class="toc-link">Why This Matters</a></li>
                    <li><a href="#partial" class="toc-link">1. Partial Derivatives</a></li>
                    <li><a href="#gradient" class="toc-link">2. The Gradient ($\nabla$)</a></li>
                    <li><a href="#directional" class="toc-link">3. Directional Derivatives</a></li>
                    <li><a href="#jacobian" class="toc-link">4. The Jacobian Matrix</a></li>
                    <li><a href="#landscape" class="toc-link">5. The Loss Landscape</a></li>
                    <li><a href="#hessian" class="toc-link">6. The Hessian Matrix</a></li>
                    <li><a href="#latent" class="toc-link">7. Geometry of Latent Space</a></li>
                    <li><a href="#python" class="toc-link">8. Python Code</a></li>
                    <li><a href="#lagrange" class="toc-link">9. Lagrange Multipliers</a></li>
                    <li><a href="#pde" class="toc-link">10. Intro to PDEs</a></li>
                    <li><a href="#autodiff" class="toc-link">11. How Computers Actually Do This: Autodiff</a></li>
                    <li><a href="#summary" class="toc-link">12. Summary</a></li>
                    <li><a href="#videos" class="toc-link">13. Video Lessons</a></li>
                    <li><a href="#practice" class="toc-link">14. Practice Problems</a></li>
                </ul>
            </nav>
            <div class="stats-card">
                <div class="toc-title">Your Progress</div>
                <div class="stats-row"><span class="stats-label">Attempted</span><span class="stats-value"
                        id="statAttempted">0</span></div>
                <div class="stats-row"><span class="stats-label">Correct</span><span class="stats-value"
                        id="statCorrect">0</span></div>
                <div class="stats-row"><span class="stats-label">Accuracy</span><span class="stats-value"
                        id="statAccuracy">‚Äî</span></div>
            </div>
        </aside>

        <main class="content">
            <!-- Why This Matters -->
            <section class="section" id="why">
                <div class="section-header">
                    <div class="section-number">Section 0</div>
                    <h2 class="section-title">Why Multivariable Matters</h2>
                </div>
                <div class="section-body">
                    <p>
                        A neural network doesn't just have one weight. It has billions.
                        We need to know how changing ONE specific weight affects the Total Error.
                    </p>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>üß† Machine Learning</h3>
                            <p>Loss = f(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô) where n can be billions.</p>
                            <p>Gradient = direction to update all weights at once.</p>
                        </div>
                        <div class="card">
                            <h3>üéÆ Computer Graphics</h3>
                            <p>Surface normals = gradients of implicit surfaces.</p>
                            <p>Ray marching uses gradients to find intersections.</p>
                        </div>
                    </div>

                    <div class="info">
                        <span class="info-icon">üó∫Ô∏è</span>
                        <strong>Analogy: The Topographic Map</strong>
                        <br>Imagine a mountain with height z = f(x, y).
                        <br>Partial ‚àÇf/‚àÇx = how steep going East?
                        <br>Partial ‚àÇf/‚àÇy = how steep going North?
                        <br>Gradient ‚àáf = which direction is steepest uphill?
                    </div>
                </div>
            </section>

            <!-- Partial -->
            <section class="section" id="partial">
                <div class="section-header">
                    <div class="section-number">Section 1</div>
                    <h2 class="section-title">Partial Derivatives</h2>
                </div>
                <div class="section-body">
                    <div class="rule">
                        <strong>The Rule:</strong>
                        <br>To find $\frac{\partial f}{\partial x}$ (Partial with respect to x):
                        <br>1. Pretend all other variables (y, z, etc.) are Constants.
                        <br>2. Take the derivative normally with respect to x.
                    </div>

                    <div class="info">
                        <strong>Example:</strong> $f(x, y) = x^2y + 3y$.
                        <br>‚Ä¢ $\frac{\partial f}{\partial x}$: Treat $y$ as a number. Deriv of $x^2y$ is $2xy$. Result:
                        $2xy$.
                        <br>‚Ä¢ $\frac{\partial f}{\partial y}$: Treat $x$ as a number. Deriv of $x^2y + 3y$ is $x^2 + 3$.
                    </div>

                    <div class="info">
                        <span class="info-icon">üì∑</span>
                        <strong>Analogy: The Camera Sliders</strong>
                        <br>Imagine a photo editor with brightness (x) and contrast (y) sliders.
                        <br>‚àÇf/‚àÇx = "If I only move the brightness slider, how does the image change?"
                        <br>You're isolating the effect of one variable.
                    </div>
                </div>
            </section>

            <!-- Gradient -->
            <section class="section" id="gradient">
                <div class="section-header">
                    <div class="section-number">Section 2</div>
                    <h2 class="section-title">The Gradient Vector ($\nabla$)</h2>
                </div>
                <div class="section-body">
                    <p>The Gradient is just a Vector containing all the partial derivatives.</p>
                    <div class="math-block">
                        $$ \nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \\
                        \vdots \end{bmatrix} $$
                    </div>

                    <div class="grid-2col">
                        <div class="info">
                            <span class="info-icon">‚¨ÜÔ∏è</span>
                            <strong>Points Uphill:</strong>
                            <br>The Gradient always points in the direction of steepest increase.
                            <br>Its magnitude = how steep the slope is.
                        </div>
                        <div class="info">
                            <span class="info-icon">‚¨áÔ∏è</span>
                            <strong>Gradient Descent:</strong>
                            <br>To minimize loss, go opposite: $-\nabla f$
                            <br>This is the "downhill" direction!
                        </div>
                    </div>
                </div>
            </section>

            <!-- Directional Derivatives -->
            <section class="section" id="directional">
                <div class="section-header">
                    <div class="section-number">Section 3</div>
                    <h2 class="section-title">Directional Derivatives</h2>
                </div>
                <div class="section-body">
                    <p>What if you want to know the slope in a direction that's NOT along an axis?</p>

                    <div class="rule">
                        <strong>Directional Derivative:</strong>
                        <br>$D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u}$
                        <br>Where $\mathbf{u}$ is a unit vector in the direction you care about.
                        <br>This is just the dot product of the gradient with your direction!
                    </div>

                    <div class="info">
                        <span class="info-icon">üéø</span>
                        <strong>Analogy: The Skier's Choice</strong>
                        <br>You're on a mountain. The gradient points steepest downhill.
                        <br>But you can ski in any direction $\mathbf{u}$.
                        <br>$D_{\mathbf{u}} f$ tells you how fast you'll descend if you ski that way.
                    </div>
                </div>
            </section>

            <!-- Jacobian -->
            <section class="section" id="jacobian">
                <div class="section-header">
                    <div class="section-number">Section 4</div>
                    <h2 class="section-title">The Jacobian Matrix</h2>
                </div>
                <div class="section-body">
                    <p>
                        What if the Input is a Vector AND the Output is a Vector?
                        Then the derivative is a Matrix.
                    </p>

                    <div class="rule">
                        <strong>Structure:</strong>
                        <br>Rows = number of outputs, Cols = number of inputs.
                        <br>$J_{ij} = \frac{\partial y_i}{\partial x_j}$
                    </div>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>ü§ñ Robotics</h3>
                            <p>The Jacobian tells you: "If I tweak motor angles slightly, how does the hand (X,Y,Z)
                                move?"</p>
                        </div>
                        <div class="card">
                            <h3>üß† Neural Networks</h3>
                            <p>Each layer's Jacobian tells you how its outputs change w.r.t. inputs.</p>
                            <p>Backpropagation = chain rule with Jacobians!</p>
                        </div>
                    </div>

                    <div class="info">
                        <span class="info-icon">üîó</span>
                        <strong>Deep Dive: Backpropagation & Chain Rule</strong>
                        <br>In a deep network, you have layers $f_3(f_2(f_1(x)))$.
                        <br>The derivative of the final output with respect to the input is the <strong>Product of
                            Jacobians</strong>:
                        <br>$$\frac{\partial y}{\partial x} = J_3 \cdot J_2 \cdot J_1$$
                        <br>This is why we call it "Backpropagation"‚Äîwe propagate errors backward through these matrix
                        products.
                    </div>
                </div>
            </section>

            <!-- Loss Landscape -->
            <section class="section" id="landscape">
                <div class="section-header">
                    <div class="section-number">Section 5</div>
                    <h2 class="section-title">The Loss Landscape</h2>
                </div>
                <div class="section-body">
                    <p>
                        Training an AI is just navigating a multivariable function.
                        We want to find the $W$ that minimizes $L(W)$.
                    </p>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>1. Flat vs. Sharp Minima</h3>
                            <p>A "Sharp" minimum (high Hessian curvature) is brittle. A "Flat" minimum generalizes
                                better to new data.</p>
                        </div>
                        <div class="card">
                            <h3>2. The Saddle Point Problem</h3>
                            <p>In high dimensions, Local Minima are rare! Most critical points are Saddle Points (one
                                direction goes down, one goes up). Gradient descent can get "stuck" here.</p>
                        </div>
                    </div>

                    <div class="info">
                        <span class="info-icon">‚õ∑Ô∏è</span>
                        <strong>Analogy: The Foggy Mountain</strong>
                        <br>Imagine you are on a mountain (the Loss) in thick fog. You can only see the ground beneath
                        your feet.
                        <br>Checking the slope with your foot is calculating the <strong>Gradient</strong>.
                        <br>Stepping downhill is <strong>Gradient Descent</strong>.
                        <br>The "Fog" is the high dimensionality‚Äîyou don't know if this path leads to the valley or a
                        cliff until you take it.
                    </div>
                </div>
            </section>

            <!-- Hessian -->
            <section class="section" id="hessian">
                <div class="section-header">
                    <div class="section-number">Section 6</div>
                    <h2 class="section-title">The Hessian Matrix</h2>
                </div>
                <div class="section-body">
                    <p>The Hessian is the matrix of second-order partial derivatives.</p>

                    <div class="math-block">
                        $$ H = \begin{bmatrix} \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x
                        \partial y} \\
                        \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} \end{bmatrix} $$
                    </div>

                    <div class="grid-2col">
                        <div class="info">
                            <span class="info-icon">üìà</span>
                            <strong>Curvature Info:</strong>
                            <br>Tells you about the "shape" of the surface.
                            <br>Eigenvalues determine if a point is a min, max, or saddle.
                        </div>
                        <div class="info">
                            <span class="info-icon">üöÄ</span>
                            <strong>Newton's Method:</strong>
                            <br>Uses Hessian for faster optimization.
                            <br>$x_{n+1} = x_n - H^{-1} \nabla f$
                            <br>Converges much faster than gradient descent!
                        </div>
                    </div>
                </div>
            </section>

            <!-- Latent Space -->
            <section class="section" id="latent">
                <div class="section-header">
                    <div class="section-number">Section 7</div>
                    <h2 class="section-title">The Geometry of Latent Space</h2>
                </div>
                <div class="section-body">
                    <p>
                        In LLMs (like GPT), words aren't characters; they are <strong>Vectors</strong> in a
                        12,288-dimensional space.
                        Calculating the "direction" between concepts is pure multivariable calculus.
                    </p>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>Concept Arithmetic</h3>
                            <p>$\text{Vector}(\text{"King"}) - \text{Vector}(\text{"Man"}) +
                                \text{Vector}(\text{"Woman"}) \approx \text{Vector}(\text{"Queen"})$</p>
                        </div>
                        <div class="card">
                            <h3>Gradient Attraction</h3>
                            <p>During training, the gradient "pulls" related concepts together in this high-dimensional
                                manifold.</p>
                        </div>
                    </div>

                    <div class="info">
                        <span class="info-icon">üåå</span>
                        <strong>CS Analogy: The Global Namespace</strong>
                        <br>Imagine a software project with millions of variables.
                        Usually, you can't just "add" two variables to get a third (e.g., `User` + `Admin` doesn't make
                        sense).
                        <br>In a <strong>Latent Space</strong>, every point is a valid state. You can "interpolate"
                        between two points to find a hybrid state.
                        Calculus allows us to move through this namespace smoothly.
                    </div>
                </div>
            </section>

            <!-- Python -->
            <section class="section" id="python">
                <div class="section-header">
                    <div class="section-number">Section 8</div>
                    <h2 class="section-title">Python: Gradients & Optimization</h2>
                </div>
                <div class="section-body">
                    <div class="grid-2col">
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-lang">Python</span>
                                <span class="code-label">Hessian & Eigenvalues with NumPy</span>
                            </div>
                            <div class="code-content">
                                <pre><code class="language-python">import numpy as np

# f(x, y) = x^2 - y^2 (Saddle Point)
# Grad = [2x, -2y]
# Hessian = [[2, 0], [0, -2]]

H = np.array([[2, 0], [0, -2]])
eigenvals = np.linalg.eigvals(H)

print(f"Eigenvalues: {eigenvals}")
# One positive, one negative -> Confirmed Saddle Point!</code></pre>
                            </div>
                        </div>
                        <div class="code-block">
                            <div class="code-header">
                                <span class="code-lang">Python</span>
                                <span class="code-label">Autograd in PyTorch</span>
                            </div>
                            <div class="code-content">
                                <pre><code class="language-python">import torch

# Create a 'Latent Vector'
v = torch.randn(128, requires_grad=True)

# Define a Dummy Loss
target = torch.ones(128)
loss = torch.nn.functional.mse_loss(v, target)

# BACKPROPAGATION
loss.backward()

print(f"First 5 Gradients: {v.grad[:5]}")
# Tells the optimizer which way to nudge 'v' 
# to make it more like 'target'</code></pre>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Lagrange -->
            <section class="section" id="lagrange">
                <div class="section-header">
                    <div class="section-number">Section 9</div>
                    <h2 class="section-title">Constrained Optimization: Lagrange Multipliers</h2>
                </div>
                <div class="section-body">
                    <p>
                        In the real world, we don't just minimize loss. We minimize loss <strong>subject to
                            constraints</strong> (e.g., hardware power limits, budget, or fairness).
                    </p>

                    <div class="rule">
                        <strong>The Method:</strong>
                        <br>To find max/min of $f(x, y)$ subject to $g(x, y) = c$:
                        <br>Solve $\nabla f = \lambda \nabla g$
                        <br>This means the gradients must be parallel at the optimal point!
                    </div>

                    <div class="info">
                        <span class="info-icon">‚öñÔ∏è</span>
                        <strong>CS Connection: Regularization</strong>
                        <br>In Machine Learning, <strong>L2 Regularization (Weight Decay)</strong> is effectively a
                        Lagrange Multiplier problem.
                        <br>We want to minimize the error, but we "punish" the model if the weights get too large.
                    </div>
                </div>
            </section>

            <!-- PDE -->
            <section class="section" id="pde">
                <div class="section-header">
                    <div class="section-number">Section 10</div>
                    <h2 class="section-title">Intro to PDEs (Physics Engines)</h2>
                </div>
                <div class="section-body">
                    <p>
                        A <strong>Partial Differential Equation (PDE)</strong> is an equation where the unknown is a
                        multivariable function, and it's constrained by its own partial derivatives.
                    </p>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>üéÆ Game Physics</h3>
                            <p>Water, smoke, and cloth are simulated by solving the <strong>Navier-Stokes PDEs</strong>.
                            </p>
                        </div>
                        <div class="card">
                            <h3>üîä Audio/Image</h3>
                            <p>The <strong>Wave Equation</strong> and <strong>Heat Equation</strong> are used for signal
                                denoising and sound propagation.</p>
                        </div>
                    </div>

                    <div class="warning">
                        <span class="warning-icon">‚ö†Ô∏è</span>
                        <strong>Complexity Warning:</strong>
                        <br>Most PDEs cannot be solved with pen and paper. We use <strong>Finite Element Methods
                            (FEM)</strong> or <strong>Neural PDE Solvers</strong> to approximate them.
                    </div>
                </div>
            </section>

            <!-- Autodiff -->
            <section class="section" id="autodiff">
                <div class="section-header">
                    <div class="section-number">Section 11</div>
                    <h2 class="section-title">How Computers Actually Do This: Autodiff</h2>
                </div>
                <div class="section-body">
                    <p>
                        Does PyTorch do partial derivatives using "Formulas"? <strong>No.</strong>
                        Does it use "Numerical limits" ($f(x+h) - f(x)/h$)? <strong>No.</strong>
                    </p>

                    <div class="info">
                        <strong>Automatic Differentiation</strong> is a way to compute exact derivatives using the
                        "Computational Graph."
                        Every operation (addition, sin, exp) is a node that knows its local Jacobian. The computer just
                        multiplies them using the Chain Rule.
                    </div>

                    <div class="grid-2col">
                        <div class="card">
                            <h3>1. Forward Mode</h3>
                            <p>Great when you have few inputs and many outputs (Robotics).</p>
                        </div>
                        <div class="card">
                            <h3>2. Reverse Mode</h3>
                            <p>The core of Deep Learning. Great when you have millions of inputs (weights) and one
                                output (loss).</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Summary -->
            <section class="section" id="summary">
                <div class="section-header">
                    <div class="section-number">Section 12</div>
                    <h2 class="section-title">Summary</h2>
                </div>
                <div class="section-body">
                    <div class="grid-2col">
                        <div class="rule">
                            <strong>Derivatives:</strong>
                            <br>‚Ä¢ <strong>Partial:</strong> Derivative w.r.t. one variable
                            <br>‚Ä¢ <strong>Gradient:</strong> Vector of all partials
                            <br>‚Ä¢ <strong>Directional:</strong> Derivative in any direction
                        </div>
                        <div class="rule">
                            <strong>Matrices:</strong>
                            <br>‚Ä¢ <strong>Jacobian:</strong> First derivatives (vector ‚Üí vector)
                            <br>‚Ä¢ <strong>Hessian:</strong> Second derivatives (curvature)
                            <br>‚Ä¢ Both are key to optimization algorithms
                        </div>
                    </div>

                    <div class="warning">
                        <strong>Common Gotchas:</strong>
                        <br>‚Ä¢ <strong>Notation:</strong> ‚àÇ (partial) vs d (total) derivative matters!
                        <br>‚Ä¢ <strong>Order:</strong> Mixed partials are equal (Clairaut's theorem).
                        <br>‚Ä¢ <strong>Dimensions:</strong> Gradient is n√ó1, Jacobian is m√ón, Hessian is n√ón.
                    </div>
                </div>
            </section>


            <!-- Videos -->
            <section class="section" id="videos">
                <div class="section-header">
                    <div class="section-number">Section 13</div>
                    <h2 class="section-title">Video Lessons</h2>
                </div>
                <div class="section-body">
                    <div class="video-container">
                        <div class="video-player">
                            <iframe id="ytPlayer" src="" allowfullscreen></iframe>
                            <div class="video-info">
                                <div class="video-title" id="videoTitle">Select a video</div>
                                <div class="video-meta" id="videoMeta"></div>
                            </div>
                        </div>
                        <div class="video-list" id="videoList"></div>
                    </div>
                </div>
            </section>

            <!-- Practice -->
            <section class="section" id="practice">
                <div class="section-header">
                    <div class="section-number">Section 14</div>
                    <h2 class="section-title">Practice Problems</h2>
                </div>
                <div class="section-body">
                    <div class="quiz-controls">
                        <button class="btn btn-primary" id="btnCheckAll">Check All</button>
                        <button class="btn" id="btnRevealAll">Reveal All</button>
                        <button class="btn" id="btnClear">Clear Inputs</button>
                        <button class="btn btn-danger" id="btnReset">Reset Stats</button>
                    </div>
                    <div id="quizContainer"></div>
                </div>
            </section>
        </main>
    </div>

    <script>
        const VIDEO_GROUPS = [
            {
                title: "Multivariable Basics",
                items: [
                    { title: "Partial Derivatives Intro", channel: "Professor Leonard", vid: "S1Te842WnxY" },
                    { title: "Gradient Vector Explained", channel: "Khan Academy", vid: "GkB4vW16QHI" },
                    { title: "Gradient Descent Visualized", channel: "3Blue1Brown", vid: "IHZwWFHWa-w" },
                    { title: "Directional Derivatives", channel: "Khan Academy", vid: "N_ZRcLheNv0" }
                ]
            },
            {
                title: "Jacobian & Hessian",
                items: [
                    { title: "Jacobian Matrix Explained", channel: "Steve Brunton", vid: "Bq6q48XQadA" },
                    { title: "Jacobian in ML", channel: "Mutual Information", vid: "wCZ-zQdMjk4" },
                    { title: "Hessian Matrix", channel: "Khan Academy", vid: "Yc45OxskMOU" },
                    { title: "Second Derivative Test", channel: "Professor Leonard", vid: "MKQvMBNk_RA" }
                ]
            },
            {
                title: "Applications",
                items: [
                    { title: "Vector Calculus Overview", channel: "3Blue1Brown", vid: "rB83DpBJQsE" },
                    { title: "Newton's Method", channel: "StatQuest", vid: "sDv4f4s2SB8" },
                    { title: "Backpropagation & Jacobians", channel: "Andrej Karpathy", vid: "VMj-3S1tku0" },
                    { title: "Robotics & Jacobians", channel: "Angela Sodemann", vid: "VKoHYVr7wtI" }
                ]
            }
        ];

        initLesson({
            videos: VIDEO_GROUPS,
            questions: window.QUESTIONS_DATA['day17'] || [],
            storageKey: 'day17_v2'
        });
    </script>
    <script>hljs.highlightAll();</script>
</body>

</html>