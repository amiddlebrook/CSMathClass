<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Day 53: Estimation, MLE & Bias-Variance</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500;600;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../shared-styles.css">
    <script>(function () { const t = localStorage.getItem('math_cs_theme') || (window.matchMedia('(prefers-color-scheme:dark)').matches ? 'dark' : 'light'); document.documentElement.setAttribute('data-theme', t); })();</script>
</head>

<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../../index.html" class="nav-back">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M10 12L6 8L10 4" />
                </svg>
                Back to Curriculum
            </a>
            <div class="nav-progress">
                <span id="progressText">0 / 10 complete</span>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
            </div>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme"></button>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-label">
            <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                <path d="M8 0L10 6H16L11 9.5L13 16L8 12L3 16L5 9.5L0 6H6L8 0Z" />
            </svg>
            Day 53 · Probability & Statistics
        </div>
        <h1>Estimation & Inference</h1>
        <p class="hero-desc">
            How do we learn parameters from data? From the Frequentist approach (Confidence Intervals)
            to the optimization view (Maximum Likelihood) and the fundamental Bias-Variance Tradeoff.
        </p>
        <div class="hero-meta">
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <circle cx="12" cy="12" r="10" />
                    <path d="M12 6v6l4 2" />
                </svg>
                ~55 min read
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path
                        d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2" />
                </svg>
                10 practice problems
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <polygon points="5 3 19 12 5 21 5 3" />
                </svg>
                Video lessons
            </div>
        </div>
    </header>

    <div class="main-layout">
        <aside class="sidebar">
            <nav class="toc">
                <div class="toc-title">On this page</div>
                <ul class="toc-list">
                    <li><a href="#estimators" class="toc-link">1. Estimators</a></li>
                    <li><a href="#mle" class="toc-link">2. Maximum Likelihood</a></li>
                    <li><a href="#bias" class="toc-link">3. Bias-Variance</a></li>
                    <li><a href="#inference" class="toc-link">4. Statistical Inference</a></li>
                    <li><a href="#python" class="toc-link">5. Python MLE</a></li>
                    <li><a href="#videos" class="toc-link">6. Video Lessons</a></li>
                    <li><a href="#practice" class="toc-link">7. Practice</a></li>
                </ul>
            </nav>
            <div class="stats-card">
                <div class="toc-title">Your Progress</div>
                <div class="stats-row"><span class="stats-label">Attempted</span><span class="stats-value"
                        id="statAttempted">0</span></div>
                <div class="stats-row"><span class="stats-label">Correct</span><span class="stats-value"
                        id="statCorrect">0</span></div>
                <div class="stats-row"><span class="stats-label">Accuracy</span><span class="stats-value"
                        id="statAccuracy">—</span></div>
            </div>
        </aside>

        <main class="content">
            <!-- Estimators -->
            <section class="section" id="estimators">
                <div class="section-header">
                    <div class="section-number">Section 1</div>
                    <h2 class="section-title">Point Estimators</h2>
                </div>
                <div class="section-body">
                    <p>
                        We have data $x_1, ..., x_n$ from a distribution with unknown parameter $\theta$.
                        An <strong>estimator</strong> $\hat{\theta}$ is a function of the data that guesses $\theta$.
                    </p>

                    <div class="subsection">
                        <h3 class="subsection-title">Properties</h3>
                        <div class="list-group">
                            <div class="list-item">
                                <strong>Unbiasedness:</strong> $E[\hat{\theta}] = \theta$. On average, it hits the
                                target.
                                <br>Example: Sample mean $\bar{X}$ is unbiased for $\mu$.
                            </div>
                            <div class="list-item">
                                <strong>Consistency:</strong> $\hat{\theta} \to \theta$ as $n \to \infty$. More data =
                                better answer.
                            </div>
                            <div class="list-item">
                                <strong>Efficiency:</strong> Lowest possible variance among unbiased estimators
                                (Cramér-Rao bound).
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- MLE -->
            <section class="section" id="mle">
                <div class="section-header">
                    <div class="section-number">Section 2</div>
                    <h2 class="section-title">Maximum Likelihood (MLE)</h2>
                </div>
                <div class="section-body">
                    <p>
                        The most common way to design estimators in CS and AI.
                        "Choose the parameters that maximize the probability of observing the data we actually saw."
                    </p>

                    <div class="rule">
                        <strong>Likelihood Function:</strong> $L(\theta; x) = P(x | \theta)$
                        <br>
                        <strong>Log-Likelihood:</strong> $\ell(\theta) = \ln L(\theta)$. (Easier to differentiate sums
                        than products).
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Example: Bernoulli MLE</h3>
                        <p>Data: sequence of H, T, H... with $k$ heads in $n$ flips. Parameter $p$.</p>
                        <div class="math-block">L(p) = p^k (1-p)^{n-k}
                            \ell(p) = k \ln p + (n-k) \ln (1-p)

                            Take derivative w.r.t p and set to 0:
                            \frac{k}{p} - \frac{n-k}{1-p} = 0
                            k(1-p) = p(n-k)
                            k - kp = pn - pk
                            k = pn \implies \hat{p}_{MLE} = \frac{k}{n}</div>
                        <p>Result: The intuitive "sample proportion" is exactly the MLE!</p>
                    </div>

                    <div class="info">
                        <strong>MAP Estimation:</strong> If we have a <em>Prior</em> distribution on $\theta$, we
                        maximize $P(\theta|x) \propto P(x|\theta)P(\theta)$.
                        This is "Maximum A Posteriori". It introduces regularization (bias) to reduce variance.
                    </div>
                </div>
            </section>

            <!-- Bias-Variance -->
            <section class="section" id="bias">
                <div class="section-header">
                    <div class="section-number">Section 3</div>
                    <h2 class="section-title">Bias-Variance Tradeoff</h2>
                </div>
                <div class="section-body">
                    <p>
                        The fundamental tension in supervised learning.
                    </p>
                    <div class="formula-box">Mean Squared Error = Bias^2 + Variance + Irreducible Error</div>

                    <div class="two-col">
                        <div>
                            <h4>Bias (Underfitting)</h4>
                            <p>Error from erroneous assumptions (e.g., assuming data is linear when it's quadratic).</p>
                        </div>
                        <div>
                            <h4>Variance (Overfitting)</h4>
                            <p>Error from sensitivity to small fluctuations in the training set.</p>
                        </div>
                    </div>
                    <div class="math-block">High Bias: Model is too simple (misses signal)
                        High Variance: Model is too complex (fits noise)
                        Total Error is convex; we want the sweet spot.</div>
                </div>
            </section>

            <!-- Inference -->
            <section class="section" id="inference">
                <div class="section-header">
                    <div class="section-number">Section 4</div>
                    <h2 class="section-title">Statistical Inference</h2>
                </div>
                <div class="section-body">
                    <p>Quantifying uncertainty about our estimates.</p>

                    <div class="subsection">
                        <h3 class="subsection-title">Confidence Intervals</h3>
                        <p>An interval $[L, U]$ that covers the true parameter $95\%$ of the time in repeated
                            experiments.</p>
                        <div class="math-block">\bar{x} \pm 1.96 \frac{\sigma}{\sqrt{n}} \quad (95\% \text{ for Normal})
                        </div>
                    </div>

                    <div class="subsection">
                        <h3 class="subsection-title">Hypothesis Testing (Null Hypothesis Significance Testing)</h3>
                        <div class="rule">
                            <strong>p-value:</strong> The probability of observing data at least as extreme as yours,
                            assuming the Null Hypothesis ($H_0$) is true.
                        </div>
                        <ul>
                            <li>Low p-value (< 0.05) $\implies$ Reject $H_0$. Data is surprising under null.</li>
                            <li>High p-value $\implies$ Fail to reject. Data is consistent with null.</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Python -->
            <section class="section" id="python">
                <div class="section-header">
                    <div class="section-number">Section 5</div>
                    <h2 class="section-title">Python MLE</h2>
                </div>
                <div class="section-body">
                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">Numerical MLE with SciPy</span>
                        </div>
                        <div class="code-content">
                            <pre><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> scipy.optimize <span class="code-keyword">import</span> minimize
<span class="code-keyword">from</span> scipy <span class="code-keyword">import</span> stats

<span class="code-comment"># Generate synthetic data from Poisson(lambda=5)</span>
true_lambda = <span class="code-number">5</span>
data = np.random.poisson(true_lambda, size=<span class="code-number">100</span>)

<span class="code-comment"># Define negative log-likelihood function (to minimize)</span>
<span class="code-keyword">def</span> <span class="code-function">neg_log_likelihood</span>(params, data):
    lam = params[<span class="code-number">0</span>]
    <span class="code-keyword">if</span> lam <= <span class="code-number">0</span>: <span class="code-keyword">return</span> np.inf <span class="code-comment"># lambda must be positive</span>
    <span class="code-comment"># P(X=k) = e^-lam * lam^k / k!</span>
    <span class="code-comment"># log P = -lam + k*log(lam) - log(k!)</span>
    <span class="code-comment"># Sum over all data points</span>
    log_lik = -lam * <span class="code-function">len</span>(data) + np.<span class="code-function">sum</span>(data) * np.log(lam)
    <span class="code-keyword">return</span> -log_lik

<span class="code-comment"># Optimize</span>
result = minimize(neg_log_likelihood, x0=[<span class="code-number">1.0</span>], args=(data,))
mle_lambda = result.x[<span class="code-number">0</span>]

<span class="code-function">print</span>(<span class="code-string">f"True Lambda: {true_lambda}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"MLE Estimate: {mle_lambda:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Sample Mean:  {np.mean(data):.4f}"</span>)
<span class="code-comment"># For Poisson, MLE is exactly the sample mean.</span></pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Videos -->
            <section class="section" id="videos">
                <div class="section-header">
                    <div class="section-number">Section 6</div>
                    <h2 class="section-title">Video Lessons</h2>
                </div>
                <div class="section-body">
                    <div class="video-container">
                        <div class="video-player">
                            <iframe id="ytPlayer" src=""
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                allowfullscreen></iframe>
                            <div class="video-info">
                                <div class="video-title" id="videoTitle">Select a video to start</div>
                                <div class="video-meta" id="videoMeta"></div>
                            </div>
                        </div>
                        <div class="video-list" id="videoList"></div>
                    </div>
                </div>
            </section>

            <!-- Practice -->
            <section class="section" id="practice">
                <div class="section-header">
                    <div class="section-number">Section 7</div>
                    <h2 class="section-title">Practice Problems</h2>
                </div>
                <div class="section-body">
                    <div class="quiz-controls">
                        <button class="btn btn-primary" id="btnCheckAll">Check All</button>
                        <button class="btn" id="btnRevealAll">Reveal All</button>
                        <button class="btn" id="btnClear">Clear Inputs</button>
                        <button class="btn btn-danger" id="btnReset">Reset Stats</button>
                    </div>
                    <div id="quizContainer"></div>
                </div>
            </section>
        </main>
    </div>

    <script src="../shared-scripts.js"></script>
    <script>
        const VIDEO_GROUPS = [
            {
                title: "Inference & MLE",
                items: [
                    { title: "Maximum Likelihood Estimation", channel: "StatQuest", vid: "Xcn85Dn10zs" },
                    { title: "Bias-Variance Tradeoff", channel: "StatQuest", vid: "EuBBz3bI-aA" },
                    { title: "Confidence Intervals", channel: "3Blue1Brown", vid: "None" }, // 3b1b no CI video, maybe Khan
                    { title: "p-values", channel: "StatQuest", vid: "vemZtEM63GY" }
                ]
            }
        ];
        const QUESTIONS = [
            { topic: "MLE", prompt: "MLE maximizes the ___ function.", answer: "likelihood" },
            { topic: "Log-Likelihood", prompt: "We use log-likelihood because it turns products into ___.", answer: "sums" },
            { topic: "Bias", prompt: "Difference between E[estimator] and truth is ___.", answer: "bias" },
            { topic: "Variance", prompt: "Sensitivity to training data fluctuations is ___.", answer: "variance" },
            { topic: "Tradeoff", prompt: "High bias usually means ___fitting.", answer: "under" },
            { topic: "Tradeoff", prompt: "High variance usually means ___fitting.", answer: "over" },
            { topic: "Consistency", prompt: "Estimator converges to truth as n -> infinity is ___.", answer: "consistency" },
            { topic: "Bernoulli MLE", prompt: "MLE for Bernoulli p is k/___.", answer: "n" },
            { topic: "p-value", prompt: "Small p-value suggests rejecting the ___ hypothesis.", answer: "null" },
            { topic: "MAP", prompt: "MAP includes a ___ distribution.", answer: "prior" }
        ];
        initLesson({ videos: VIDEO_GROUPS, questions: QUESTIONS, storageKey: 'day53_est_v2' });
    </script>
</body>

</html>