<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Day 26: Matrix Decompositions (SVD)</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&family=DM+Sans:wght@400;500;600;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body);"></script>
    <link rel="stylesheet" href="../../lessons/shared-styles.css">
    <script src="../../lessons/questions-data.js"></script>
    <script src="../../lessons/shared-scripts.js"></script>
</head>

<body>
    <nav class="nav">
        <div class="nav-inner">
            <a href="../../index.html" class="nav-back">
                <svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M10 12L6 8L10 4" />
                </svg>
                Back to Curriculum
            </a>
            <div class="nav-progress">
                <span id="progressText">0 / 27 complete</span>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
            </div>
            <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" />
                </svg>
            </button>
        </div>
    </nav>

    <header class="hero">
        <div class="hero-label">
            <svg width="16" height="16" viewBox="0 0 16 16" fill="currentColor">
                <path d="M8 0L10 6H16L11 9.5L13 16L8 12L3 16L5 9.5L0 6H6L8 0Z" />
            </svg>
            Day 26 · The Atoms of Data
        </div>
        <h1>Singular Value Decomposition (SVD)</h1>
        <p class="hero-desc">
            Primes are the atoms of numbers ($14 = 2 \times 7$).
            SVD is the atoms of Matrices.
            It breaks ANY matrix into 3 simple steps: Rotate, Stretch, Rotate.
        </p>
        <div class="hero-meta">
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <circle cx="12" cy="12" r="10" />
                    <path d="M12 6v6l4 2" />
                </svg>
                ~110 min read
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path
                        d="M9 5H7a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2V7a2 2 0 00-2-2h-2M9 5a2 2 0 002 2h2a2 2 0 002-2M9 5a2 2 0 012-2h2a2 2 0 012 2" />
                </svg>
                40 practice problems
            </div>
            <div class="meta-item">
                <svg class="meta-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <polygon points="5 3 19 12 5 21 5 3" />
                </svg>
                12 video lessons
            </div>
        </div>
    </header>

    <div class="main-layout">
        <aside class="sidebar">
            <nav class="toc">
                <div class="toc-title">On this page</div>
                <ul class="toc-list">
                    <li><a href="#definition" class="toc-link">1. The Master Algorithm</a></li>
                    <li><a href="#geometry" class="toc-link">2. Deep Dive: Geometry</a></li>
                    <li><a href="#low-rank" class="toc-link">3. Low-Rank Approx</a></li>
                    <li><a href="#recommender" class="toc-link">4. CS: Recommender Sys</a></li>
                    <li><a href="#python" class="toc-link">5. Python: Compression</a></li>
                    <li><a href="#summary" class="toc-link">6. Summary</a></li>
                    <li><a href="#videos" class="toc-link">7. Video Lessons</a></li>
                    <li><a href="#practice" class="toc-link">8. Practice Problems</a></li>
                </ul>
            </nav>
            <div class="stats-card">
                <div class="toc-title">Your Progress</div>
                <div class="stats-row"><span class="stats-label">Attempted</span><span class="stats-value"
                        id="statAttempted">0</span></div>
                <div class="stats-row"><span class="stats-label">Correct</span><span class="stats-value"
                        id="statCorrect">0</span></div>
                <div class="stats-row"><span class="stats-label">Accuracy</span><span class="stats-value"
                        id="statAccuracy">—</span></div>
            </div>
        </aside>

        <main class="content">
            <!-- Definition -->
            <section class="section" id="definition">
                <div class="section-header">
                    <div class="section-number">Section 1</div>
                    <h2 class="section-title">The Master Algorithm</h2>
                </div>
                <div class="section-body">
                    <p>
                        Eigenvectors are great, but they are fussy. They generally require square matrices ($n \times
                        n$).
                        <br><strong>SVD</strong> is the universal tool. It works for <strong>ANY</strong> matrix, square
                        or rectangular ($m \times n$).
                    </p>
                    <div class="card">
                        <h3>The Decomposition</h3>
                        <p>Any matrix $A$ can be broken down into three specific matrices:</p>
                        <div class="math-block">
                            $$ A = U \Sigma V^T $$
                        </div>
                        <ul>
                            <li><strong>$U$</strong> (Left Singular Vectors): orthogonal matrix ($m \times m$).</li>
                            <li><strong>$\Sigma$</strong> (Sigma): diagonal matrix of "Singular Values" ($m \times n$).
                            </li>
                            <li><strong>$V^T$</strong> (Right Singular Vectors): orthogonal matrix ($n \times n$).</li>
                        </ul>
                    </div>
                    <div class="subsection">
                        <h3>Analogy: The Prism</h3>
                        <p>
                            Imagine white light hitting a prism. The prism breaks the messy mixed light into pure
                            spectral components (Red, Green, Blue).
                            <br>SVD is the mathematical prism. It takes a messy matrix $A$ and breaks it into pure,
                            independent components ($\sigma_1, \sigma_2, ...$).
                        </p>
                    </div>
                </div>
            </section>

            <!-- Geometry -->
            <section class="section" id="geometry">
                <div class="section-header">
                    <div class="section-number">Section 2</div>
                    <h2 class="section-title">Deep Dive: The Geometry</h2>
                </div>
                <div class="section-body">
                    <p>
                        What does a matrix $A$ <em>do</em> to a vector $\vec{x}$? ($A\vec{x}$).
                        <br>SVD tells us it happens in three distinct steps, read from right to left ($V^T \to \Sigma
                        \to U$).
                    </p>
                    <div class="rule">
                        <strong>Rotate $\to$ Stretch $\to$ Rotate</strong>
                        <ol>
                            <li><strong>$V^T$ (Rotate)</strong>: First, we align the input vector to the principal axes.
                            </li>
                            <li><strong>$\Sigma$ (Stretch)</strong>: Then, we stretch along those axes. The amount of
                                stretch is given by the singular values $\sigma_1, \sigma_2...$. Some directions might
                                get squashed to zero.</li>
                            <li><strong>$U$ (Rotate)</strong>: Finally, we rotate the result to the output orientation.
                            </li>
                        </ol>
                    </div>
                    <p>
                        Visualizing this: A unit circle gets rotated, then stretched into an ellipse, then rotated
                        again.
                        <br>The singular values $\sigma$ are the lengths of the semi-axes of that ellipse.
                    </p>
                </div>
            </section>

            <!-- Compression -->
            <section class="section" id="low-rank">
                <div class="section-header">
                    <div class="section-number">Section 3</div>
                    <h2 class="section-title">Low-Rank Approximation</h2>
                </div>
                <div class="section-body">
                    <p>
                        This is the "Killer App" of SVD.
                        <br>The singular values $\sigma$ are ordered by size: $\sigma_1 \ge \sigma_2 \ge ... \ge 0$.
                    </p>
                    <div class="info">
                        <strong>Signal vs Noise</strong>
                        <br>The large values represent the meaningful structure (the "Signal").
                        <br>The tiny tail values represent random fluctuations (the "Noise" or "Static").
                    </div>
                    <p>
                        If we throw away the small values (set them to 0) and reconstruct the matrix, we get the
                        <strong>Best Possible Approximation</strong> of the original matrix for that amount of data.
                    </p>
                    <div class="rule">
                        <strong>The Eckart-Young-Mirsky Theorem</strong>
                        <br>To compress a matrix $A$:
                        <br>1. Compute SVD.
                        <br>2. Keep only top $k$ singular values.
                        <br>3. Multiply back: $A_k = U_k \Sigma_k V_k^T$.
                    </div>
                </div>
            </section>

            <!-- Recommender -->
            <section class="section" id="recommender">
                <div class="section-header">
                    <div class="section-number">Section 4</div>
                    <h2 class="section-title">CS: Recommender Systems</h2>
                </div>
                <div class="section-body">
                    <p>
                        Think of the Netflix problem.
                        <br><strong>Rows:</strong> Users.
                        <br><strong>Columns:</strong> Movies.
                        <br><strong>Values:</strong> Ratings (1-5 stars).
                    </p>
                    <p>
                        This matrix is huge and mostly empty (sparse). SVD finds the hidden structure.
                    </p>
                    <div class="card">
                        <h3>Latent Factors</h3>
                        <p>
                            SVD discovers hidden concepts ("Latent Factors") that explain the ratings.
                        </p>
                        <ul>
                            <li><strong>$U$ (Users vs Concepts):</strong> How much does User A like "Sci-Fi"?</li>
                            <li><strong>$\Sigma$ (Concept Strength):</strong> How important is "Sci-Fi" in the dataset?
                            </li>
                            <li><strong>$V^T$ (Concepts vs Movies):</strong> How much is "Star Wars" a "Sci-Fi" movie?
                            </li>
                        </ul>
                        <p>
                            To predict if User A will like "Star Wars", we just check if their vectors align in this
                            Concept Space.
                        </p>
                    </div>
                </div>
            </section>

            <!-- Python -->
            <section class="section" id="python">
                <div class="section-header">
                    <div class="section-number">Section 5</div>
                    <h2 class="section-title">Python: Image Compression</h2>
                </div>
                <div class="section-body">
                    <p>
                        Let's perform Low-Rank Approximation on a real image.
                        We will treat a grayscale image as a matrix of numbers (pixels).
                    </p>

                    <div class="code-block">
                        <div class="code-header">
                            <span class="code-lang">Python</span>
                            <span class="code-label">SVD Compression</span>
                        </div>
                        <div class="code-content">
                            <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_sample_image

# 1. Load an image (China or Flower)
img = load_sample_image("china.jpg")
# Convert to Grayscale for simplicity (Average of RGB)
img_gray = img.mean(axis=2)

print(f"Original Shape: {img_gray.shape} (H, W)")

# 2. Compute SVD
# This creates U, Sigma (vector), and Vt
U, S, Vt = np.linalg.svd(img_gray, full_matrices=False)

print(f"Singular Values found: {len(S)}")

# 3. Compress!
# Function to reconstruct using only top 'k' singular values
def compress(k):
    # We only take the first k columns of U, first k values of S, first k rows of Vt
    # The @ operator is matrix multiplication
    reconst = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
    return reconst

# 4. Compare Ranks
k_values = [5, 20, 50, 427] # 427 is full rank here

plt.figure(figsize=(12, 6))
for i, k in enumerate(k_values):
    plt.subplot(1, 4, i+1)
    plt.imshow(compress(k), cmap='gray')
    plt.title(f"Rank {k}\n({(k/len(S)*100):.1f}% data)")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Notice how Rank 50 looks almost perfect, despite using ~10% of the data.
# That is the power of SVD.</code></pre>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Summary -->
            <section class="section" id="summary">
                <div class="section-header">
                    <div class="section-number">Section 6</div>
                    <h2 class="section-title">Summary</h2>
                </div>
                <div class="section-body">
                    <ul>
                        <li><strong>SVD</strong> breaks ANY matrix into <strong>Rotation</strong>,
                            <strong>Stretch</strong>, <strong>Rotation</strong>.
                        </li>
                        <li><strong>Singular Values ($\sigma$)</strong> represent the "Energy" or importance of each
                            component.</li>
                        <li><strong>Low-Rank Approximation</strong> keeps the largest $\sigma$'s to denoise or compress
                            data.</li>
                        <li>This is the math behind <strong>Image Compression</strong>, <strong>Recommender
                                Systems</strong>, and data reduction.</li>
                    </ul>
                </div>
            </section>


            <!-- Videos -->
            <section class="section" id="videos">
                <div class="section-header">
                    <div class="section-number">Section 7</div>
                    <h2 class="section-title">Video Lessons</h2>
                </div>
                <div class="section-body">
                    <div class="video-container">
                        <div class="video-player">
                            <iframe id="ytPlayer" src="" allowfullscreen></iframe>
                            <div class="video-info">
                                <div class="video-title" id="videoTitle">Select a video</div>
                                <div class="video-meta" id="videoMeta"></div>
                            </div>
                        </div>
                        <div class="video-list" id="videoList"></div>
                    </div>
                </div>
            </section>

            <!-- Practice -->
            <section class="section" id="practice">
                <div class="section-header">
                    <div class="section-number">Section 8</div>
                    <h2 class="section-title">Practice Problems</h2>
                </div>
                <div class="section-body">
                    <div class="quiz-controls">
                        <button class="btn btn-primary" id="btnCheckAll">Check All</button>
                        <button class="btn" id="btnRevealAll">Reveal All</button>
                        <button class="btn" id="btnClear">Clear Inputs</button>
                        <button class="btn btn-danger" id="btnReset">Reset Stats</button>
                    </div>
                    <div id="quizContainer"></div>
                </div>
            </section>
        </main>
    </div>

    <script>
        const VIDEO_GROUPS = [
            {
                title: "The Main Event",
                items: [
                    { title: "SVD Overview", channel: "Steve Brunton", vid: "gXbThCXjZFM" },
                    { title: "SVD Mathematical Overview", channel: "Steve Brunton", vid: "nbBvuuNVfco" },
                    { title: "Singular Values", channel: "Steve Brunton", vid: "vSczTbgc8Rc" },
                    { title: "Matrix Approximation", channel: "Steve Brunton", vid: "xy3QyyhiuY4" }
                ]
            },
            {
                title: "Applications: Compression",
                items: [
                    { title: "Image Compression with SVD", channel: "Steve Brunton", vid: "DG7YTlGnCEo" },
                    { title: "Least Squares and SVD", channel: "Steve Brunton", vid: "GtbtI5-asE6" },
                    { title: "PCA vs SVD", channel: "StatQuest", vid: "FgakZw5KKua8" }
                ]
            },
            {
                title: "Applications: Data Science",
                items: [
                    { title: "Recommender Systems", channel: "StatQuest", vid: "Eeg1DEeWUjA" },
                    { title: "Latent Semantic Analysis", channel: "Victor Lavrenko", vid: "P5mlg91as1c" },
                    { title: "Pseudoinverse", channel: "Steve Brunton", vid: "PjeOmOz9jSY" },
                    { title: "Essence of Linear Algebra", channel: "3Blue1Brown", vid: "fNk_zzaMoSs" }
                ]
            }
        ];


        initLesson({
            videos: VIDEO_GROUPS,
            questions: window.QUESTIONS_DATA['day26'] || [],
            storageKey: 'day26_v2'
        });
    </script>
    <script>hljs.highlightAll();</script>
</body>

</html>